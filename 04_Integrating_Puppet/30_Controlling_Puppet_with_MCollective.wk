= Controlling Puppet with MCollective =

* MCollective can be used to:
* Query, stop, start, and restart the Puppet agent.
* Run the Puppet agent with special command-line options.
* Query and make changes to the node using Puppet resources.
* Choose nodes to act on based on Puppet classes or facts on the node.
* Control concurrency of Puppet runs across a group of nodes.

== Configuring MCollective ==

=== Enabling the Puppet Labs Repository ===

* Install the repository as follows:

<source lang="bash">
sudo yum install http://yum.puppetlabs.com/puppetlabs-release-el-7.noarch.rpm
</source>

=== Installing the MCollective Module ===

* Install the MCollective module from Puppet Forge.

<source lang="bash">
cd /etc/puppetlabs/code/environments/production
puppet module install jorhett-mcollective --modulepath=modules/
</source>

=== Generating Passwords ===

* Four strings are required for authentication.

# '''Client Password:''' Used by the MCollective client to authenticate with the middleware.
# '''Server Password:''' Used by nodes to authenticate with the middleware.
# '''Preshared Salt:''' Used by clients and servers to validate that requests have arrived intact.
# '''Java Keystore Password:''' Use to protect a Java keystore.

* The server credentials installed on nodes will allow them to subscribe to command channels, but not send commands.
* If you use the same credentails for clients and servers, anyone with access to a server's configuration file will have control of the entire collective.
* Create 4 long, random strings, and copy them to a text editor.

<source lang="bash">
openssl rand -base64 32
openssl rand -base64 32
openssl rand -base64 32
openssl rand -base64 32
</source>

* The pre-shared key model will work, but is only useful for demonstrative purposes.
* TLS security plugins can be used to encrypt the transport of keys for full cryptographic authentication.

=== Configuring Hiera for MCollective ===

* For testing, put the following Hiera configuration in <code>common.yaml</code>.

<pre>
# Every node installs the server.
classes:
  - mcollective::server

# The Puppet Server will host the middleware.
mcollective::hosts:
  - 'puppet.example.com'
mcollective::collectives:
  - 'mcollective'
mcollective::connector: 'activemq'
mcollective::connector_ssl: true
mcollective::connector_ssl_type: 'anonymous'

# Access passwords.
mcollective::server_password: 'Server Password'
mcollective::psk_key: 'Pre-Shared Salt'

mcollective::facts::cronjob::run_every: 10
mcollective::server::package_ensure: 'latest'
mcollective::plugins::agents:
  puppet:
    version: 'latest'

mcollective::client::unix_group: vagrant
mcollective::client::package_ensure: 'latest'
mcollective::plugins::clients:
  puppet:
    version: 'latest'
</pre>

* Every node will install and enable <code>mcollective::server</code>.
* The remaining values identify the connection type.

=== Enabling the Middleware ===

* For the purpose of testing, we can install the middleware on the puppetserver VM.
* Requires minimal resources.
* The firewall on the middleware node must allow connections on 61614.

<source lang="bash">
sudo firewall-cmd --permanent --zone=public --add-port=61614/tcp
sudo firewall-cmd reload
</source>

* Add Hiera data for the Puppet server that sets the MCollective classes and configuration.

'''/etc/puppetlabs/code/hieradata/hostname/puppetserver.yaml'''

<pre>
classes:
  - mcollective::middleware
  - mcollective::client

mcollective::client_password: 'Client Password'
mcollective::middleware::keystore_password: 'Keystore Password'
mcollective::middleware::truststore_password: 'Keystore Password'
</pre>

* Installs ActiveMQ middleware and client on the Puppet server.
* Run Puppet to configure the node.

<source lang="bash">
sudo puppet agent --test
</source>

=== Connecting MCollective Servers ===

* Each node must run the Puppet agent again to configure MCollective.
* Verify that MCollective has established a connection with the Puppet server.

<source lang="bash">
netstat -an | grep 61614
</source>

=== Validating the Installation ===

* All nodes should be online and connected to the middleware.
* This can be verified with the <code>mco ping</code> command on the Puppet server.
* Troubleshooting
* Verify the middleware host allows connections on port 61614.
* The middleware host doesn't hav the same server password as the nodes.
** Check <code>/var/log/messages</code> on the servers for authentication errors.
* The middleware host doesn't have the same client password as your client.
** Ensure the same value for <code>client_password</code> is used on both the client and middleware host.

=== Creating Another Client ===

* You only need to install the client software on systems from which you will be sending requests.
* Management hosts, or an administrator workstation.
* Create a per-host Hiera data file, and set the MCollective class/configuration.

'''/etc/puppetlabs/code/hieradata/hostname/client.yaml'''

<pre>
classes:
  - mcollective:client

mcollective::client_password: 'Client Password'
</pre>

* Run Puppet agent to enable MCollective.
* You can optionally tune which group has read access to the <code>client.cfg</code> file, reducing risk of malicious users obtaining the salt used to validate requests to the hosts.

<pre>
mcollective::client::unix_group: 'vagrant'
</pre>

=== Installing MCollective Agents and Clients ===

* The MCollective Puppet agent is installed automatically using the previously mentioned Hiera data files.
* Ensure the Puppet agent plugin is installed on all servers, and the Puppet client plugin is installed on all clients.

<pre>
mcollective::plugins::agents:
  puppet:
    version: 'latest'

mcollective::client::unix_group: vagrant
mcollective::client::package_ensure: 'latest'
mcollective::plugins::clients:
  puppet:
    version: 'latest'
</pre>

=== Sharing Facts with Puppet ===

* Node facts can be made available to MCollective.
* Useful to filter the list of nodes that should act on a request.
* Set the <code>mcollective::facts::cronjob::run_every</code> parameter.
* Number of minutes between updates of the file.
* This causes <code>/etc/puppetlabs/mcollective/facts.yaml</code> to be populated with all Facter and Puppet-provided facts.
* You can use the <code>mco inventory</code> request to read through all facts available on a node.

<source lang="bash">
mco inventory client.example.com | awk '/Facts:/','/^$'
</source>

* You can query for how many nodes share the same value for facts.

<source lang="bash">
$ mco facts operatingsystem
Report for fact: kernel

    Linux    found 2 times
    FreeBSD  found 1 times

Finished processing 3 / 3 hosts in 61.45 ms
</pre>

== Pulling the Puppet Strings ==

=== Viewing Node Inventory ===

* This command allows you to see how a given server is configured.
* What collectives it is part of.
* What facts it has.
* What Puppet classes are applied.
* Running statistics.

<source lang="bash">
mco inventory client.example.com
</source>

* You can pass ruby scripts to the inventory service as well, for formatting the output in different ways.

'''~/inventory.mc'''

<source lang="ruby">
# Format: hostname: architecture, operating system, OS release.
inventory do
  format "%20s: %8s %10s %20s"
  fields {[
    identity,
    facts["os.architecture"],
    facts["operatingsystem"],
    facts["operatingsystemrelease"]
  ]}
end
</source>

* Call the inventory command and pass the script file.

<source lang="bash">
mco inventory --script inventory.mc
</source>

=== Checking Puppet Status ===

* You can query the Puppet agent status of any node using MCollective.
* First, find a list of nodes with the MCollective Puppet agent installed.

<source lang="bash">
mco find --with-agent puppet
</source>

* Now check the currnt agent status.

<source lang="bash">
mco puppet count
</source>

* A graphical summary of nodes can be output as well.

<source lang="bash">
mco puppet summary
</source>

=== Disabling the Puppet Agent ===

* You may wish to disable the Puppet agent during maintenance periods, such as patching.
* An optional message can be added to explain why the disable took place.

<source lang="bash">
mco puppet disable --with-identity client.example.com message="Patching"
</source>

* If someone tries to run Puppet on the node, they will get a message back explaining that it is disabled.

<source lang="bash">
mco puppet runonce --with-identity client.example.com
</source>

* Once ready, the agent can then be re-enabled.

<source lang="bash">
mco puppet enable --with-identity client.example.com
</source>

=== Invoking Ad Hoc Puppet Runs ===

* The Puppet agent can be instructed to evaluate the catalog immediately on a node.

<source lang="bash">
mco puppet runonce --with-identity client.example.com
</source>

* You can also target multiple nodes by fact values.

<source lang="bash">
mco puppet runonce --tags=sudo --with-fact operatingsystem=CentOS
</source>

* Or, you can run all servers in batches specified by a number input.

<source lang="bash">
# Run all, 5 at a time.
mco puppet runall 5
</source>

* If there is an issue with overlap during the above, a sleep time can be introduced between batches when calling the <code>--batch</code> option instead of <code>runall</code>.

<source lang="bash">
mco puppet --batch 10 --batch-sleep 60 --tags ntp
</source>

=== Limiting Targets with Filters ===

* Filters are used by the discovery plugin to limit which nodes are sent a request.
* Filters can be applied to any MCollective command.
* '''Example:''' List all nodes with "serv" in the host FQDN.

<source lang="bash">
mco find --with-identity /serv/
</source>

* '''Example:''' List hosts with the Puppet class mcollective::client.

<source lang="bash">
mco find --with-class mcollective::client
</source>

* '''Example:''' List hosts with the operatingsystem fact set as Ubuntu.

<source lang="bash">
mco find --with-fact operatingsystem=Ubuntu
</source>

* There are two types of combination filters.
* Combine Puppet classes and Facter facts.
* '''Example:''' Find all CentOS hosts with the puppet class 'nameserver' applied.

<source lang="bash">
mco find --with "/nameserver/ operatingsystem=CentOS"
</source>

* Create searches with the select filter against facts and Puppet classes.
* Includes Boolean logic.
* '''Example:''' All CentOS hosts that are not in the test environment.

<source lang="bash">
mco find --select "operatingsystem=CentOS and not environment=test"
</source>

* '''Example:''' Virtualized hosts with either the httpd or nginx Puppet class applied.

<source lang="bash">
mco find --select "(/httpd/ or /ngnx/) and is_virtual=true"
</source>

=== Providing a List of Targets ===

* You can specify which nodes to make requests of using a file with one FQDN per line.

<source lang="bash">
mco puppet runonce --disc-method flatfile --disc-option /tmp/list-of-names.host
</source>

* Alternatively, you can pipe the list of nodes to the command:

<source lang="bash">
cat list-of-hosts.txt | mco puppet runonce -disc-method stdin
</source>

=== Limiting Concurrency ===

* By default, every node which matches the query will respond immediately on contact.
* Flags in the command can be used to set how many servers receive a request in a batch, and how much time to wait between batches.

<source lang="bash">
# Requests a response from one (effectively random) node.
mco puppet status --one
</source>

<source lang="bash">
# A fixed number of servers or a percentage of servers matching a filter.
mco puppet status --limit 2
</source>

<source lang="bash">
# One-third of nodes with the webserver Puppet class applied.
# Tell them to return their FQDN.
mco shell run "hostname -fqdn" --limit 33% --with-class webserver.
</source>

=== Manipulating Puppet Resource Types ===

* You can use MCollective to interact directly with each node's RAL.

<source lang="bash">
mco puppet resource service httpd ensure=stopped --with-identity /dashserver/
</source>

* The normal rules for using filters apply.